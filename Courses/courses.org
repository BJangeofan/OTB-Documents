#+TITLE: Exploit Pleiades PHR data with the ORFEO ToolBox
#+AUTHOR: Manuel Grizonnet (CNES), Julien Michel (CNES)
#+OPTIONS: H:4
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper,11pt,twoside,openright]
#+LaTeX_HEADER: \usepackage{a4wide}
#+LaTeX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \usepackage{mathptmx}
#+LaTeX_HEADER: \usepackage[scaled=.90]{helvet}
#+LaTeX_HEADER: \usepackage{courier}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{tikz}
#+OPTIONS: tags:nil

#+EXPORT_SELECT_TAGS:
#+EXPORT_EXCLUDE_TAGS: classpix stereo

#+LATEX:\pagestyle{fancy}
#+LATEX:\fancyhf{}
#+LATEX:\fancyhead[LE,RO]{\bfseries\thepage}
#+LATEX:\fancyhead[LO]{\bfseries\rightmark}
#+LATEX:\fancyhead[RE]{\bfseries\leftmark}
#+LATEX:\fancyfoot[LE,RO]{\vspace{0pt}\includegraphics[height=20pt]{../Artwork/logoVectoriel.png}}
#+LATEX:\fancyfoot[LO,RE]{\vspace{0pt}\includegraphics[height=20pt]{Images/logo_cnes.png}}
#+LATEX:\fancyfoot[C]{\vspace{2pt}\footnotesize{OGRS2012 - OTB Workshop}}

* Foreword                                                               :fw:
** About the data

  The images used during these exercise are extracts from Pleiades
  demonstration products, made available for evaluation purpose. To
  get the full products please refer to this [[http://www.astrium-geo.com/fr/364-produits-de-demonstration][website]]. Products used
  are: 
  - ~Pleiades Pan-sharpened ORTHO Compression REGULAR~
  - ~Pleiades TRISTEREO Bundle PRIMARY~

  They are covered by a cnes copyright.

  Other data needed for some exercises, as well as solution scripts
  can be found in the data package.

** About the software

   To perform the exercises, you will need to have the following
   software installed:
   - *Orfeo ToolBox* 3.14 or later, including applications
   - *Monteverdi* 1.6 or later
   - *QGis* 1.8 or later

     For *Orfeo ToolBox* and *Monteverdi* installation, you can refer to
     the installation from the [[http://www.orfeo-toolbox.org/packages/OTBSoftwareGuide.pdf][Orfeo ToolBox Cookbook]].

     For *QGis* installation, please refer to *QGis* documentation,
     which can be found on the project [[http://www.qgis.org/][website]].
     
* Exercises                                                            :exos:
#+INCLUDE: "mvdapps.org"
#+INCLUDE: "prepro.org"
#+INCLUDE: "seg.org"

** Learning and classification from pixels                        :classpix:
*** Description
**** Abstract

     This exercise will get you familiar with the OTB pixel based
     classification applications. You will learn how to train a SVM
     classification model from Pleiades images and a set of training
     regions. You will then learn how to apply this model to images
     and produce shiny classification maps.

**** Pre-requisites
     
     - Basic knowledge on OTB applications and QGis usage
     - Basic knowledge on image supervised classification
     - Basic knowledge on GIS vector file formats

**** Achievements

     - Usage of the OTB Classification applications
     - Classification of large images
     - Import of results in a GIS software

*** Steps

    In this part of the exercise, you will use the following data:

    ~melbourne_ms_toa_ortho_extract_small.tif~

**** Produce and analyze learning samples

     - Use Qgis to produce polygons for 5 classes (vegetation, roads, soil, buildings and water)
     - Export this vector layer in shapefile
     - What is the label corresponding to the class *water* in the
       shapefile?  An example set of learning samples is provided for
       the exercise in /training.shp/

     _Tips and Recommendations:_
      - Note the field name of the shapefile which contains the label. You will
        need to provide this field in the training application

**** Estimate image statistics

     In order to make these features comparable between each images,
     the first step is to estimate the input images statistics. These
     statistics will be used to center and reduce the intensities
     (mean of 0 and standard deviation of 1) of training samples from
     the vector data produced by the user.

     - Use the *ComputeImagesStatistics* to compute statistics on
       the image
     - What is the mean of the red band?
     - The extract provided has been converted from DN to
       milli-reflectance. For what reasons, is it advised to do so
       when performing multiple images classification?

**** Estimate classification model using the Support Vector Machine algorithm
     
     The *TrainSVMImagesClassifier* application performs SVM
     classifier training from multiple pairs of input images and
     training vector data. Samples are composed of pixel values in
     each band optionally centered and reduced using XML statistics
     file produced by the *ComputeImagesStatistics* application. We will
     use this application with only one image in this exercise.
     - Use the *TrainSVMImagesClassifier* to produce SVM model
     - Which kernel is used by default in the application?
     - What is the measured accuracy?

**** Apply classification model
     - Use the *ImageSVMClassifier* to apply the classification model to the input image
     - What is the output of the application?
     - Bonus : Use the same model to apply the classification to the other extract
 
       ~melbourne_ms_toa_ortho_extract_large.tif~

**** Produce printable classification map
     We are now going to produce a printable classification map using the
     *ColorMapping* application This tool will replace each label with an 8-bits
     RGB color specified in a mapping file.  The mapping file should look like
     this :
     
     : $ # Lines beginning with a # are ignored
     : 1 255 0 0

     - Produce your custom look-up table (LUT)
     - Use this LUT to produce a printable classification map (in PNG format)
     - Overlay this map on the input image in QGIS. Comment on the classification results.

**** Homework

     - Produce classification model with different kind of SVM
       kernels. Comment different accuracies obtained?
     - Going big: Apply this classification on the pan-sharpened image over Melbourne

** Learning and classification from objects                       :classobj:
*** Description
**** Abstract

     This workshop will introduce you to the *Object Labeling* module
     of *Monteverdi*. You will learn how to use the module and see the
     influence of different features on classification results. You
     will also experiment with a simple active learning implementation
     on objects.

**** Pre-requisites

     - Basic knowledge of Object Based Image Analysis
     - Basic knowledge on learning and classification

**** Achievements

     Being able to use the *Object Labeling* module of *Monteverdi*.

*** Steps

**** The preliminary segmentation

     In this part of the exercise, we will use the following data:

     ~phr_pxs_melbourne_xt_small.tif~
     
     ~phr_pxs_melbourne_xt_small_segmentation.tif~

     1. Use the *ColorMapping* application to enhance the
        visualization of the segmented image (you can use the
        /optimal/ and /image/ modes as learned in the segmentation
        exercise).

     2. Analyze the color-mapped segmentation results. For which kind
        of objects is the object based classification likely to work
        well ? For which kind of objects is it likely to perform badly?

**** *Object Labeling* module - basics

     1. Open both the image and the segmentation image in *Monteverdi*.
     2. Open the *Object Labeling* module from the /learning/ menu,
        and load the image and the segmentation inside the module.
     3. What is the purpose of each tab on the left side of the module?
     4. In the /Objects/ tab, create a new class. You can change its
        color and its name.
     5. Right-click on an object of interest in the image. What
        happens?
     6. Right-click a second time inside the selected object. What
        happens?
     7. Add a few more objects to the current class.
     8. Create a new class and add some objects to it.
     9. Go to the /Features/ tab, uncheck all features but the mean
        radiometric values.
     10. Go to the /Learning/ tab and click on classify. What happens?
     11. Click on the /Save/Quit/ button. What kind of outputs is
         produced by the module?

     _Tips and Recommendations:_
     - Choose two simple classes for this part of the exercise (for
       instance a /Water/ class and a /Land/ class)
     - Use the navigation map to change the displayed area
     - You can change the opacity of the classification layer as well
       as of the selected objects layer so as to better analyze the
       results.
     - You can also clear the classification layer.     

**** *Object Labeling* module - advanced

     In this part of the exercise, we will use these additional files:
     ~samples.xml~ and ~parameters.xml~

     1. Load again the image and the segmentation inside the module.

     2. Load the samples file using /File/Load Samples/. What are the
        different object classes loaded ? How many samples per classes
        are used ?

     3. Uncheck all features except from radiometric means:
        - Band1::Mean
        - Band2::Mean
        - Band3::Mean
        - Band4::Mean

     4. Perform the classification. What are the objects in the image
        that are badly classified because of missing classes ?

     5. What are the objects in the image that are poorly classified
        because they are badly segmented or too complex ?

     6. Try to enhance the classification by adding missing classes.

     7. Try to enhance the classification by adding new features.

     _Tips and Recommendations:_
     - The *Object Labeling* module is quite memory
       consuming. Depending on the available memory on your system,
       you might want to restart *Monteverdi*.

**** *Object Labeling* module - active learning

     1. In the /Objects/ tab, click on the /Sample/ button in the
        lower-left area. This will show you difficult samples by using
        the /margin sampling/ technique.

     2. What kind of segments are considered by the algorithm as hard
        to classify ?

     3. Try to create a /Trash/ class to handle noise segments.

     4. Perform a few more iteration of active learning. What do you
        observe ?
** Elevation map from stereo pair                                   :stereo:
*** Description
**** Abstract

     This exercise will guide get you familiar with the set of OTB
     applications which allow to compute elevation map from a stereo
     pair of optical images.  You will learn how to :
     - re-sample for stereo pair in epipolar geometry to reduce the
       stereo correspondences to a 1D problem
     - Perform block matching between the 2 images to extract the
       disparity (related to the elevation)
     - Filter disparities using correlation metric analysis
       
**** Pre-requisites

     - Basic knowledge on OTB applications
     - Basic knowledge on [[http://www.ai.sri.com/~luong/research/Meta3DViewer/EpipolarGeo.html][epipolar geometry]]. Epipolar geometry is the
       geometry of stereo vision (see [[http://en.wikipedia.org/wiki/Epipolar_geometry][here]]). The operation of stereo
       rectification determines transformations to apply to each image
       such that pairs of conjugate epipolar lines become collinear,
       parallel to one of the image axes and aligned. In this
       geometry, the objects on a given row of the left image are also
       located on the same line in the right image.

     #+Latex:\vspace{0.5cm}
     #+Latex:\begin{center}
     #+ATTR_LaTeX: width=0.45\textwidth
     #+CAPTION: Epipolar geometry
     [[file:Images/Epipolar_geometry.png]]
     #+Latex:\end{center}

     - Basic knowledge of stereoscopic reconstruction

**** Achievements

     - Usage of stereoscopic reconstruction applications
     - Stereo reconstruction based on Pleiades stereo images pair

*** Steps
    
**** From images to epipolar geometry

     In this part of the exercise, you will use the following data:
     ~tristereo_melbourne_1_small.tif and tristereo_melbourne_2_small.tif~

     1. Run the command-line and graphical version of the
        *StereoRectificationGridGenerator* application
     2. What are the two outputs of the applications?
     3. Use the application to generate two re-sampling grids. Which OTB application
        allows to resample the two input images using these grids?
     4. Use this application to resample input stereo pair in
        epipolar geometry, open the 2 re-sampled images. What do you
        see ?

     _Tips and Recommandations:_
       - Perform the grids estimation using and average elevation of
         20.45m (*epi.elevation.avg.value* keyword)
       - Stereo-rectification deformation grid only varies
         slowly. Therefore, it is recommended to use a coarser grid
         (higher step value) in case of large images (*epi.step*
         keyword)
       - Note the size of the images in epipolar geometry (output by
         the application)

**** Improvement of epipolar geometry
     
     Pleiades data can be orthorectified to absolute accuracies of
     about 10 meters, as a consequence there is still a need to
     improve the geometric accuracy (this is the case for all
     satellite imagery). For orthorectification purpose, it is
     achieved by optimizing sensor modelling with Ground Control
     Points. An other way to do this in case of superimposition of
     multiple images is to produce homologous points on each images
     and refine with these points the co-localisation function. It
     allows to improve the geometric accuracy and to produce
     consistent epipolar images.

     We provide for the next questions a refined version of the stereo pair:
     
     ~tristereo_melbourne_1_small_ref.tif~

     ~tristereo_melbourne_2_small_ref.tif~

     1. Recompute epipolar geometry with the new stereo pair
        (post-fixed by /\_ref.tif/). Open the 2 versions of epipolar
        couples (total of 4 images). What differences do you notice
        between the two images pair?
     2. Combined the 2 images to create a 3D anaglyph (left image on
        the red channel and the right image on the green and blue
        channel). Visualize the anaglyph with anaglyph glasses.

     We will use this images in the next questions.

**** Block matching

     We are going to perform stereo pair block matching on the two
     images using the *BlockMatching* application.

     1. Run the command-line and graphical version of the
        *BlockMatching* application. What are the mandatory parameters?
     2. Propose manual or automatic methodologies to estimate the
        interval of disparities in vertical or horizontal direction.
     3. Use these parameters to generate a disparity map and open the
        result with Monteverdi. What do you notice?

      _Tips and Recommendations:_
        - Discard pixels with no-data (0 in our case) value using the
          parameter *-mask.nodata*
**** Advanced Block matching : refine disparity map

     We are going to try now to improve the quality of the disparity
     map using options available in the *BlockMatching*.

     1. Use the Normalized Cross Correlation and output the metric
        value using the io.outmetric option. Open the metric image,
        which values of correlation corresponds to a good disparity
        value ?
     2. Use the option mask.variancet to discard pixels whose local
        variance is too small (the size of the neighborhood is given
        by the radius parameter)
     3. Use the *BandMath* application to only keep horizontal
        disparity with high correlation value.

**** From disparity map to ground elevation

     Use the *DisparityMapToElevationMap* to transform the disparity
     map into an elevation map.
     1. At which height approximately do cricket players play in the stadium?
     2. What is approximately the height of the stadium?

     _Tips and Recommandations:_
        - Reuse the same average elevation of 20.45m
        - Bonus : produce a mask using the *BandMath* application to
          discard pixels with low correlation values using the
          parameter *io.mask*

**** Homework
     1. Try refinement steps to improve epipolar geometries (available
        soon in OTB -> 3.16 version)
     2. Perform disparity coherence analysis by comparing disparity
        maps obtained by switching the left and right images
     3. Re-compute disparity maps with sub-pixel precision block-matching
     4. Use median filter to get a smoother disparity map
