\section{Classification}\label{sec:classification}

\subsection{Pixel based classification}\label{ssec:pbclassif}

The classification in the application framework provides a supervised pixel-wise
classification chain based on learning from multiple images, and using one 
specified machine learning method like SVM, Bayes, KNN, Random Forests, Artificial 
Neural Network, and others...(see application help of 
\application{TrainImagesClassifier} for further details about all the available 
classifiers). It supports huge images through streaming and multi-threading. The 
classification chain performs a training step based on the intensities of each 
pixel as features. Please note that all the input images must have the same number 
of bands to be comparable.

\subsubsection{Statistics estimation}
In order to make these features comparable between each training images, the first 
step consists in estimating the input images statistics. These statistics will be 
used to center and reduce the intensities (mean of 0 and standard deviation of 1) 
of samples based on the vector data produced by the user. To do so, the
\application{ComputeImagesStatistics} tool can be used:

\begin{verbatim}
otbcli_ComputeImagesStatistics -il  im1.tif im2.tif im3.tif
                               -out images_statistics.xml
\end{verbatim}

This tool will compute each band mean, compute the standard deviation based on
pooled variance of each band and finally export them to an XML file.
The features statistics XML file will be an input of the following tools.

\subsubsection{Building the training data set}\label{sssec:building}

As the chain is supervised, we first need to build a training set with
positive examples of different objects of interest. These polygons
must be saved in OGR vector format supported by GDAL like ESRI
shapefile for example.

Please note that the positive examples in the vector data should have a ``Class``
field with a label value higher than 1 and coherent in each images.

You can generate the vector data set with \qgis software for
example and save it in an OGR vector format supported by \gdal (ESRI
shapefile for example). \app should be able to transform the
vector data into the image coordinate system.

\subsubsection{Performing the learning scheme}

Once images statistics have been estimated, the learning scheme is the following:
\begin{enumerate}
  \item For each input image:
  \begin{enumerate}
    \item Read the region of interest (ROI) inside the shapefile,
    \item Generate validation and training data within the ROI,
    \item Add vectors respectively to the training samples set and the validation
    samples set.
  \end{enumerate}
  \item Increase the size of the training samples set and balance it by
  generating new noisy samples from the previous ones,
  \item Perform the learning with this training set
  \item Estimate performances of the classifier on the validation samples set
  (confusion matrix, precision, recall and F-Score).
\end{enumerate}

Let us consider a SVM classification. These steps can be performed by the 
\application{TrainImagesClassifier} command-line using the following:

\begin{verbatim}
otbcli_TrainImagesClassifier -io.il      im1.tif im2.tif im3.tif
                             -io.vd      vd1.shp vd2.shp vd3.shp
                             -io.imstat  images_statistics.xml
                             -classifier svm (classifier_for_the_training)
                             -io.out     model.svm
\end{verbatim}

Additional groups of parameters are also available (see application help for
more details):
\begin{itemize}
\item \verb?-elev? Handling of elevation (DEM or average elevation)
\item \verb?-sample? Group of parameters for sampling
\item \verb?-classifier? Classifiers to use for the training, and their corresponding groups of parameters
\end{itemize}


\subsubsection{Using the classification model}
Once the classifier has been trained, one can apply the model to classify
pixel inside defined classes on a new image using the
\application{ImageClassifier} application:

\begin{verbatim}
otbcli_ImageClassifier -in     image.tif
                       -imstat images_statistics.xml
                       -model  model.svm
                       -out    labeled_image.tif
\end{verbatim}

You can set an input mask to limit the classification to the mask area with
value \textgreater 0.


\subsubsection{Validating the classification model}
The performance of the model generated by the \application{TrainImagesClassifier} 
application is directly estimated by the application itself, which displays the 
precision, recall and F-score of each class, and can generate the global confusion 
matrix as an output *.CSV file.
 
With the \application{ConputeConfusionMatrix} application, it is also possible to 
estimate the performance of a model from a classification map generated with the 
\application{ImageClassifier} application. 
This labeled image is compared to positive reference samples (either represented as 
a raster labeled image or as a vector data containing the reference classes). It 
will compute the confusion matrix and precision, recall and F-score of each class 
too, based on the 
\href{http://www.orfeo-toolbox.org/doxygen-current/classotb_1_1ConfusionMatrixCalculator.html}{ConfusionMatrixCalculator} 
class.

\begin{verbatim}
otbcli_ComputeConfusionMatrix -in                labeled_image.tif
                              -ref               vector
                              -ref.vector.in     vectordata.shp
                              -ref.vector.field  Class (name_of_label_field)
                              -out               confusion_matrix.csv
\end{verbatim}


%You can also set a DEM repository (-dem) to keep the vectordata reprojection accurate.

\subsubsection{Fancy classification results}
\label{ssec:classificationcolormapping}
Color mapping can be used to apply color transformations on the final
graylevel label image. It allows to get an RGB classification map
by re-mapping the image values to be suitable for display purposes.
One can use the \application{ColorMapping} application. This tool will
replace each label with an 8-bits RGB color specified in a mapping
file. The mapping file should look like this :

\begin{verbatim}
# Lines beginning with a # are ignored
1 255 0 0
\end{verbatim}

In the previous example, 1 is the label and 255 0 0 is a RGB color
(this one will be rendered as red). To use the mapping tool, enter
the following :

\begin{verbatim}
otbcli_ColorMapping -in                labeled_image.tif
                    -method            custom
                    -method.custom.lut lut_mapping_file.txt
                    -out               RGB_color_image.tif
\end{verbatim}

Other look-up tables (LUT) are available : standard continuous LUT,
optimal LUT, and LUT computed over a support image.

\subsubsection{Example}

We consider 4 classes: water, roads, vegetation and buildings with red roofs.
Data is available in the OTB-Data
\href{http://hg.orfeo-toolbox.org/OTB-Data/file/0fed8f4f035c/Input/Classification}{repository}
and this image is produced with the commands inside this
\href{http://hg.orfeo-toolbox.org/OTB-Applications/file/3ce975605013/Testing/Classification/CMakeLists.txt}{file}.

\begin{figure}[!h]
  \center
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/classification_chain_inputimage.jpg}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/classification_chain_fancyclassif_fusion.jpg}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/classification_chain_fancyclassif.jpg}
  \itkcaption[ExampleSVMClassif]{From left to right: Original image,
    result image with fusion (with monteverdi viewer) of original
    image and fancy classification and input image with fancy color
    classification from labeled image.}
  \label{fig:MeanShiftVectorImageFilter}
\end{figure}




\subsection{Fusion of classification maps}\label{ssec:fusionofclassifications}

After having processed several classifications of the same input image but from 
different models or methods (SVM, KNN, Random Forest,...), it is possible to make a 
fusion of these classification maps with the \application{FusionOfClassifications} 
application which uses either majority voting or the Demspter Shafer framework to 
handle this fusion. The Fusion of Classifications generates a single more robust and 
precise classification map which combines the information extracted from the input 
list of labeled images. 

The \application{FusionOfClassifications} application has the following input parameters :
\begin{itemize}
\item \verb?-il? list of input labeled classification images to fuse
\item \verb?-out? the output labeled image resulting from the fusion of the input classification images
\item \verb?-method? the fusion method (either by majority voting or by Dempster Shafer)
\item \verb?-nodatalabel? label for the no data class (default value = 0)
\item \verb?-undecidedlabel? label for the undecided class (default value = 0)
\end{itemize}


The input pixels with the nodata class label are simply ignored by the fusion process. 
Moreover, the output pixels for which the fusion process does not result in a unique 
class label, are set to the undecided value.

\subsubsection{Majority voting for the fusion of classifications}

In the Majority Voting method implemented in the \application{FusionOfClassifications} 
application, the value of each output pixel is equal to the more frequent class 
label of the same pixel in the input classification maps. However, it may happen 
that the more frequent class labels are not unique in individual pixels. In that 
case, the undecided label is attributed to the output pixels.


The application can be used like this:
\begin{verbatim}
otbcli_FusionOfClassifications  -il             cmap1.tif cmap2.tif cmap3.tif
                                -method         majorityvoting
                                -nodatalabel    0
                                -undecidedlabel 10
                                -out            MVFusedClassificationMap.tif
\end{verbatim}


Let us consider 6 independent classification maps of the same input image (Cf. left 
image in Fig. \ref{fig:MeanShiftVectorImageFilter}) generated from 6 different SVM 
models. The Fig. \ref{fig:ClassificationMapFusionApplication} represents them after 
a color mapping by the same LUT. Thus, 4 classes (water: blue, roads: gray, 
vegetation: green, buildings with red roofs: red) are observable on each of them.


\begin{figure}[!h]
  \center
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/QB_1_ortho_C1_CM.png}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/QB_1_ortho_C2_CM.png}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/QB_1_ortho_C3_CM.png}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/QB_1_ortho_C4_CM.png}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/QB_1_ortho_C5_CM.png}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/QB_1_ortho_C6_CM.png}
  \itkcaption[ExampleSVMClassifFusion]{Six fancy colored classified images to be fused, generated from 6 different SVM models.}
  \label{fig:ClassificationMapFusionApplication}
\end{figure}


As an example of the \application{FusionOfClassifications} application by 
\emph{majority voting}, the fusion of the six input classification maps 
represented in Fig. \ref{fig:ClassificationMapFusionApplication} leads to the 
classification map illustrated on the right in Fig. \ref{fig:ClassificationMapFusionApplicationMV}. 
Thus, it appears that this fusion highlights the more relevant classes among the six  
different input classifications. The white parts of the fused image correspond to 
the undecided class labels, i.e. to pixels for which there is not a unique 
majority voting.


\begin{figure}[!h]
  \center
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/classification_chain_inputimage.jpg}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/QB_1_ortho_MV_C123456_CM.png}
  \itkcaption[ExampleSVMClassifFusionMV]{From left to right: Original image, and fancy colored classified image obtained by a 
  majority voting fusion of the 6 classification maps represented in Fig. \ref{fig:ClassificationMapFusionApplication} (water: blue, roads: gray, 
vegetation: green, buildings with red roofs: red, undecided: white).}
  \label{fig:ClassificationMapFusionApplicationMV}
\end{figure}


\subsubsection{Dempster Shafer framework for the fusion of classifications}

The \application{FusionOfClassifications} application, handles another method to 
compute the fusion: the Dempster Shafer framework. 
In the \href{http://en.wikipedia.org/wiki/Dempster-Shafer\_theory}{Dempster-Shafer theory}, 
the performance of each classifier resulting in the classification maps to fuse are 
evaluated with the help of the so-called \emph{belief function} of each class label, 
which measures the degree of belief that the corresponding label is correctly assigned 
to a pixel. For each classifier, and for each class label, these belief
functions are estimated from another parameter called the \emph{mass of belief} of each class label, 
which measures the confidence that the user can have in each classifier according to 
the resulting labels.

In the Dempster Shafer framework for the fusion of classification maps, the fused 
class label for each pixel is the one with the maximal belief function. In case of 
multiple class labels maximizing the belief functions, the output fused pixels are 
set to the undecided value.

In order to estimate the confidence level in each classification map, each of them 
should be confronted with a ground truth. For this purpose, the masses of belief of 
the class labels resulting from a classifier are estimated from its confusion 
matrix, which is itself exported as a *.CSV file with the help of the 
\application{ComputeConfusionMatrix} application. Thus, using the Dempster Shafer 
method to fuse classification maps needs an additional input list of such *.CSV files 
corresponding to their respective confusion matrices.

The application can be used like this:
\begin{verbatim}
otbcli_FusionOfClassifications  -il             cmap1.tif cmap2.tif cmap3.tif
                                -method         dempstershafer
                                -method.dempstershafer.cmfl
                                                cmat1.csv cmat2.csv cmat3.csv
                                -nodatalabel    0
                                -undecidedlabel 10
                                -out            DSFusedClassificationMap.tif
\end{verbatim}



As an example of the \application{FusionOfClassifications} application by 
\emph{Dempster Shafer}, the fusion of the six input classification maps 
represented in Fig. \ref{fig:ClassificationMapFusionApplication} leads to the 
classification map illustrated on the right in Fig. \ref{fig:ClassificationMapFusionApplicationDS}. 
Thus, it appears that this fusion gives access to a more precise and robust 
classification map based on the confidence level in each classifier.


\begin{figure}[!h]
  \center
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/classification_chain_inputimage.jpg}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/QB_1_ortho_DS_V_P_C123456_CM.png}
  \itkcaption[ExampleSVMClassifFusionDS]{From left to right: Original image, and fancy colored classified image obtained by a 
  Dempster Shafer fusion of the 6 classification maps represented in Fig. \ref{fig:ClassificationMapFusionApplication} (water: blue, roads: gray, 
vegetation: green, buildings with red roofs: red, undecided: white).}
  \label{fig:ClassificationMapFusionApplicationDS}
\end{figure}
 

\subsubsection{Recommendations to properly use the fusion of classification maps}

In order to properly use the \application{FusionOfClassifications} application, 
some points should be considered. First, the \verb?list_of_input_images? and 
\verb?OutputFusedClassificationImage? are single band labeled images, which means 
that the value of each pixel corresponds to the class label it belongs to, and 
labels in each classification map must represent the same class. Secondly, the 
undecided label value must be different from existing labels in the input images in 
order to avoid any ambiguity in the interpretation of the \verb?OutputFusedClassificationImage?.





\subsection{Majority voting based classification map regularization}\label{ssec:classificationmapregularization}

Resulting classification maps can be regularized in order to smoothen irregular classes. Such a regularization process
improves classification results by making more homogeneous areas which are easier to handle.

\subsubsection{Majority voting for the classification map regularization}

The \application{ClassificationMapRegularization} application performs a regularization of a labeled input image
based on the Majority Voting method in a specified ball shaped neighborhood. For each center pixel, Majority Voting takes the
more representative value of all the pixels identified by the structuring element and then sets the output center pixel
to this majority label value. The ball shaped neighborhood is identified by its radius expressed in pixels.


\subsubsection{Handling ambiguity and not classified pixels in the majority voting based regularization}

Since, the Majority Voting regularization may lead to not unique majority labels in the neighborhood, it is important to define
which behavior the filter must have in this case. For this purpose, a Boolean parameter (called ip.suvbool) is used in the
\application{ClassificationMapRegularization} application to choose whether pixels with more than one majority class are set to
Undecided (true), or to their Original labels (false = default value).

Moreover, it may happen that pixels in the input image do not belong to any of the considered class. Such pixels are
assumed to belong to the NoData class, the label of which is specified as an input parameter for the regularization. Therefore,
those NoData input pixels are invariant and keep their NoData label in the output regularized image.
 
The \application{ClassificationMapRegularization} application has the following input parameters :
\begin{itemize}
\item \verb?-io.in? labeled input image resulting from a previous classification process
\item \verb?-io.out? output labeled image corresponding to the regularization of the input image
\item \verb?-ip.radius? integer corresponding to the radius of the ball shaped structuring element (default value = 1 pixel)
\item \verb?-ip.suvbool? boolean parameter used to choose whether pixels with more than one majority class are set to Undecided (true),
or to their Original labels (false = default value). Please note that the Undecided value must be different from existing labels in the input image
\item \verb?-ip.nodatalabel? label for the NoData class. Such input pixels keep their NoData label in the output image (default value = 0)
\item \verb?-ip.undecidedlabel? label for the Undecided class (default value = 0).
\end{itemize}


The application can be used like this:
\begin{verbatim}
otbcli_ClassificationMapRegularization  -io.in              labeled_image.tif
                                        -ip.radius          3
                                        -ip.suvbool         true
                                        -ip.nodatalabel     10
                                        -ip.undecidedlabel  7
                                        -io.out             regularized.tif
\end{verbatim}
 

\subsubsection{Recommendations to properly use the majority voting based regularization}

In order to properly use the \application{ClassificationMapRegularization} application, some points should be considered.
First, both \verb?InputLabeledImage? and \verb?OutputLabeledImage? are single band labeled images, which means that the
value of each pixel corresponds to the class label it belongs to. The \verb?InputLabeledImage? is commonly an image generated
with a classification algorithm such as the SVM classification. Remark: both
\verb?InputLabeledImage? and \verb?OutputLabeledImage? are not necessarily of the same datatype. Secondly, if ip.suvbool == true,
the Undecided label value must be different from existing labels in the input labeled image in order to avoid any ambiguity in the
interpretation of the regularized \verb?OutputLabeledImage?. Finally, the structuring element radius must have a minimum value equal to 1 pixel,
which is its default value. Both NoData and Undecided labels have a default value equal to 0.


\subsubsection{Example}

Resulting from the \application{ColorMapping} application presented in section \ref{ssec:classificationcolormapping}, and illustrated in
Fig. \ref{fig:MeanShiftVectorImageFilter}, the Fig. \ref{fig:ClassificationMapRegularizationApplication} shows a regularization of a classification
map composed of 4 classes: water, roads, vegetation and buildings with red roofs. The radius of the ball shaped structuring element is equal to 3 pixels,
which corresponds to a ball included in a 7 x 7 pixels square. Pixels with more than one majority class keep their original labels.

\begin{figure}[!h]
  \center
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/classification_chain_inputimage.jpg}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/classification_chain_fancyclassif_CMR_input.png}
  \includegraphics[width=0.3\textwidth]{../Art/MonteverdiImages/classification_chain_fancyclassif_CMR_3.png}
  \itkcaption[ExampleSVMClassifCMR]{From left to right: Original image, fancy colored classified image and regularized classification map with radius equal to 3 pixels.}
  \label{fig:ClassificationMapRegularizationApplication}
\end{figure}

\subsection{Regression}\label{ssec:regression}

The machine learning models in OpenCV and LibSVM also support a regression mode :
they can be used to predict a numeric value (i.e. not a class index) from an input
predictor. The workflow is the same as classification. First, the regression model
is trained, then it can be used to predict output values. The applications to do
that are \application{TrainRegression} and \application{PredictRegression}.

\subsubsection{Input datasets}

The input data set for training must have the following structure :
\begin{itemize}
\item \textit{n} components for the input predictors
\item one component for the corresponding output value
\end{itemize}

The \application{TrainRegression} application supports 2 input formats :
\begin{itemize}
\item An image list : each image should have components matching the structure
detailed earlier (\textit{n} feature components + 1 output value)
\item A CSV file : the first \textit{n} columns are the feature components and
the last one is the output value
\end{itemize}

If you have separate images for predictors and output values, you can use the \application{ConcatenateImages}
application.

\begin{verbatim}
otbcli_ConcatenateImages  -il features.tif  output_value.tif
                          -out training_set.tif
\end{verbatim}

\subsubsection{Statistics estimation}

As in classification, a statistics estimation step can be performed before
training. It allows to normalize the dynamic of the input predictors to a standard
one : zero mean, unit standard deviation. The main difference with the classification
case is that with regression, the dynamic of output values can also be reduced.

The statistics file format is identical to the output file from \application{ComputeImagesStatistics}
application, for instance :
\begin{verbatim}
<?xml version="1.0" ?>
<FeatureStatistics>
    <Statistic name="mean">
        <StatisticVector value="198.796" />
        <StatisticVector value="283.117" />
        <StatisticVector value="169.878" />
        <StatisticVector value="376.514" />
    </Statistic>
    <Statistic name="stddev">
        <StatisticVector value="22.6234" />
        <StatisticVector value="41.4086" />
        <StatisticVector value="40.6766" />
        <StatisticVector value="110.956" />
    </Statistic>
</FeatureStatistics>
\end{verbatim}

In the \application{TrainRegression} application, normalization of input predictors
and output values is optional. There are 3 options :
\begin{itemize}
\item No statistic file : normalization disabled
\item Statistic file with \textit{n} components : normalization enabled for input predictors only
\item Statistic file with \textit{n+1} components : normalization enabled for input predictors and output values
\end{itemize}

If you use an image list as training set, you can run \application{ComputeImagesStatistics}
application. It will produce a statistics file suitable for input and output normalization (third option).

\begin{verbatim}
otbcli_ComputeImagesStatistics  -il   training_set.tif
                                -out  stats.xml
\end{verbatim}

\subsubsection{Training}

Initially, the machine learning models in OTB only used classification. But since
they come from external libraries (OpenCV and LibSVM), the regression mode was
already implemented in these external libraries. So the integration of these models
in OTB has been improved in order to allow the usage of regression mode. As a consequence
, the machine learning models have nearly the same set of parameters for
classification and regression mode.

The regression mode is currently supported for :
\begin{itemize}
\item Support Vector Machine (LibSVM and OpenCV)
\item Decision Trees
\item Gradient Boosted Trees
\item Neural Network
\item Random Forests
\item K-Nearest Neighbors
\end{itemize}

The behaviour of \application{TrainRegression} application is very similar to
\application{TrainImagesClassifier}. From the input data set, a portion of the samples
is used for training, whereas the other part is used for validation. The user may
also set the model to train and its parameters. Once the training is done, the
model is stored in an output file.

\begin{verbatim}
otbcli_TrainRegression  -io.il                training_set.tif
                        -io.imstat            stats.xml
                        -io.out               model.txt
                        -sample.vtr           0.5
                        -classifier           knn
                        -classifier.knn.k     5
                        -classifier.knn.rule  median
\end{verbatim}


\subsubsection{Prediction}

Once the model is trained, it can be used in \application{PredictRegression} application
to perform the prediction on an entire image containing input predictors (i.e.
an image with only \textit{n} feature components). If the model was trained with
normalization, the same statistic file must be used for prediction. The behavior
of \application{PredictRegression} with respect to statistic file is identical to
\application{TrainRegression} :
\begin{itemize}
\item no statistic file : normalization off
\item \textit{n} components :  input only
\item \textit{n+1} components : input and output
\end{itemize}

The model to use is read from file (the one produced during training).

\begin{verbatim}
otbcli_PredictRegression  -in     features_bis.tif
                          -model  model.txt
                          -imstat stats.xml
                          -out    prediction.tif
\end{verbatim}



\subsection{Samples selection}\label{ssec:samplesselect}

Since release 5.4, new functionalities related to the handling of the vectors from the training data set (see also \ref{sssec:building}) were added to OTB.

The first improvement was provided by the application PolygonClassStatistics. This application processes a set of training geometries, 
and outputs statistics about the sample distribution in the input geometries (in the form of a xml file) :
\begin{itemize}
\item number of samples per class
\item number of samples per geometry
\end{itemize}

Supported geometries are polygons, lines and points; depending on the geometry type, this application behaves differently : 
\begin{itemize}
\item polygon : select pixels whose center is inside the polygon
\item lines : select pixels intersecting the line
\item points : select closest pixel to the provided point
\end{itemize}
The application also takes as input a support image, but the values of its pixels are not used.
The purpose is rather to define the image grid that will later provide the samples.
The user can also provide a raster mask, that will be used to discard pixel positions.

A simple use of the application PolygonClassStatistics could be as follows :
\begin{verbatim}
otbcli_PolygonClassStatistics  -in     support_image.tif
                               -vec    variousTrainingVectors.sqlite
                               -field  class
                               -out    polygonStat.xml
\end{verbatim}

%Once the statistics have been computed, comes the application OGRDataResampler (not yet released at the time of writing this document).
%The purpose of this application is to select samples from the training data set.

%First of all, this application needs the xml file computed by PolygonClassStatistics (parameter instats).
%Then, there are three different strategies to select samples (parameter sampstrat) :
%\begin{itemize}
%\item minnbsamp (default) : randomly select samples in such a way that the final sampled classes have the size of the smallest one.
%\item nbsampallclasses : randomly select samples in such a way that the final sampled classes have a size of N (with N below or equal to the size of the smallest class).
%\item nbsampbyclass : set the required number for each class manually with the following pattern : "class\_name1:nb\_samples1 class\_name2:nb\_samples2 ...".
%\end{itemize}
%Once the strategy is selected, the application outputs samples which are spatially uniformly distributed (parameter out).

%In addition to the parameters instats, sampstrat, and out, some others must be compulsorily specified:
%\begin{itemize}
%\item in : an input image must be provided to this application (parameter in). Here again, the values of its pixels are not used;
%the purpose is rather to provide geometry information so that the selected samples can be outputted as physical points.
%\item vec : input training vector data set, from where to pick samples.
%\item layer : index specifying from which layer to pick samples.
%\item field : set the field of the selected samples.
%\end{itemize}

%Some other optional arguments can also be passed to the application:
%\begin{itemize}
%\item mask : an optional raster mask can be used to discard samples.
%\item outrates : allows to output a txt file that summarizes the sampling rates for each class.
%\item outsampvec / insampvec : internally, this application uses a random lists of bits to decide
    %whether to select a sample or not. If users wish they could exactly repeat the selection process,
    %they should output these lists in a txt file, and reload it the next time. This is the purpose of these two parameters.
%\item maxsampvecsize : allows to limit the size of the lists of bits mentioned above, to optimize the memory footprint.
%\end{itemize}

%As with the PolygonClassStatistics application, different types  of geometry are supported : polygons, lines, points. 
%The behavior of this application is exactly the same as the one of PolygonClassStatistics.


%A simple use of the application OGRDataResampler could be as follows :
%\begin{verbatim}
%otbcli_PolygonClassStatistics  -in support_image.tif
                               %-vec variousVectors.sqlite
                               %-field class
                               %-instats apTvClPolygonClassStatisticsOut.xml
                               %-sampstrat nbsampbyclass
                               %-nbsampbyclass "1:98 2:100 3:23"
                               %-out resampledVectors.sqlite
%\end{verbatim}

%In this example, "apTvClPolygonClassStatisticsOut.xml" refers to the statistics computed by the PolygonClassStatistics application.
%The training data set from which we need to select samples is "variousVectors.sqlite". Here no mask was used, so every points had a chance to be selected.
%The sampling strategy is set to "nbsampbyclass", meaning that the user must indicate the number of samples to be selected for each class.
%This information is provided thanks to the string "1:98 2:100 3:23"; its interpretation is easy to grasp : 
%for class whose name is "1", the application mus select 98 samples, and so forth.
%The result is outputted to "resampledVectors.sqlite".

%Let's continue with another example :
%\begin{verbatim}
%otbcli_PolygonClassStatistics  -in support_image.tif
                               %-vec variousVectors.sqlite
                               %-mask some_mask.tif
                               %-field class
                               %-instats apTvClPolygonClassStatisticsOut.xml
                               %-out resampledVectors.sqlite
%\end{verbatim}

%This time, a mask was used; its pixels were interleaved with O and 1 values : it means that only fifty percents of the points could be selected.
%The parameter "sampstrat" was left empty; therefore, the used strategy was "minnbsamp" (default). 
%In this situation, the application identified the smallest class, \textit{taking into account the mask constraint} : 
%the result was class "1" with 104 samples. Then, the application uniformly and randomly selected 104 samples from classes "2" et "3".
%The result is shown in figure \ref{fig:samples1}.

%If the application is run a second time, we get another random selection for the classes "2" and "3".
%The sample selection for the class "1" didn't change : this is normal, 
%as the application will always select every point of the smallest class when using the "minnbsamp" strategy.
%The result is shown in figure \ref{fig:samples2}.



%\begin{center}
  %\begin{figure}[h!]
    %\includegraphics[width=\textwidth]{../Art/samplesselect1.png}
    %\itkcaption[First selection]{First selection.}
    %\label{fig:samples1}
   %\end{figure}
   
   %\begin{figure}[h!]
    %\includegraphics[width=\textwidth]{../Art/samplesselect2.png}
    %\itkcaption[Second selection]{Second selection.}
    %\label{fig:samples2}
   %\end{figure}
      
%\end{center}
