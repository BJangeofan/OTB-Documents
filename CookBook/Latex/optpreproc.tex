\section{Optical pre-processing}\label{sec:optpreproc}

This section presents various pre-processing tasks that are presented in
a classical order to obtain a calibrated, pan-sharpened image.

\subsection{Optical radiometric calibration}\label{ssec:optcal}

In remote sensing imagery, pixel values are called DN (for Digital
Numbers) and can not be physically interpreted and compared: they are
influenced by various factors such as the amount of light flowing
trough the sensor, the gain of the detectors and the analogic to
numeric converter.

Depending on the season, the light and atmospheric conditions, the
position of the sun or the sensor internal parameters, these DN can
drastically change for a given pixel (apart from any ground change
effects). Moreover, these effects are not uniform over the spectrum:
for instance aerosol amount and type has usually more impact on the
blue channel.

Therefore, it is necessary to calibrate the pixel values before any
physical interpretation is made out of them. In particular, this
processing is mandatory before any comparison of pixel spectrum
between several images (from the same sensor), and to train a
classifier without dependence to the atmospheric conditions at the
acquisition time.

Calibrated values are called surface reflectivity, which is a ratio
denoting the fraction of light that is reflected by the underlying
surface in the given spectral range. As such, its values lie in the
range $[0,1]$. For convenience, images are often stored in thousandth
of reflectivity, so that they can be encoded with an integer type.
Two levels of calibration are usually distinguished:

\begin{itemize}
\item The first level is called \emph{Top Of Atmosphere (TOA)}
  reflectivity. It takes into account the sensor gain, sensor spectral
  response and the solar illumination.
\item The second level is called \emph{Top Of Canopy (TOC)}
  reflectivity. In addition to sensor gain and solar illumination, it
  takes into account the optical thickness of the atmosphere, the
  atmospheric pressure, the water vapor amount, the ozone amount, as
  well as the composition and amount of aerosol gasses.
\end{itemize}

This transformation can be done either with \app or with
\mont. Sensor-related parameters such as gain, date, spectral
sensitivity and sensor position are seamlessly read from the image
metadata. Atmospheric parameters can be tuned by the user. Supported
sensors are :
\begin{itemize}
\item Pleiades,
\item SPOT5,
\item QuickBird,
\item Ikonos,
\item WorldView-1,
\item WorldView-2,
\item Formosat.
\end{itemize}

\subsubsection{Optical calibration with \app}

The \application{OpticalCalibration} application
allows to perform optical calibration. The mandatory
parameters are the input and output images. All other parameters are
optional. By default the level of calibration is set to TOA (Top Of
Atmosphere).  The output images are expressed in thousandth of
reflectivity using a 16 bits unsigned integer type.

A basic TOA calibration task can be performed with the following command :

\begin{verbatim}
otbcli_OpticalCalibration -in  input_image -out output_image
\end{verbatim}

A basic TOC calibration task can be performed with the following command :

\begin{verbatim}
otbcli_OpticalCalibration -in  input_image -out output_image -level toc
\end{verbatim}

\subsubsection{Optical calibration with \mont}

These transformations can also be done in \mont.

The 6S model needs atmospheric parameters to be able to compute
radiative terms to estimate the atmospheric contributions on the input
signal. Default parameters are available in the module.  For
atmospheric parameters, it is possible to indicate AERONET file. The
AERONET (AErosol RObotic NETwork) program is a federation of
ground-based remote sensing aerosol networks established by NASA and
PHOTONS (Univ. of Lille 1, CNES, and CNRS-INSU) and is greatly
expanded by collaborators from national agencies, institutes,
universities, individual scientists, and partners. The program
provides accessible public domain database of aerosol optical,
mircrophysical and radiative properties.

The module produces four outputs:

\begin{itemize}
\item Luminance image.
\item TOA reflectance image.
\item TOC reflectance image.
\item Difference TOA-TOC image, which allows to get the estimation of atmospheric contribution.
\end{itemize}

\begin{figure}
  \center
  \includegraphics[width=0.6\textwidth]{../Art/MonteverdiImages/monteverdi_optical_calibration.png}
  \itkcaption[GUI of the optical calibration module based on the 6S model]{Optical calibration module.}
  \label{fig:opticalcalibration}
\end{figure}


\begin{figure}
  \center
  \includegraphics[width=0.6\textwidth]{../Art/MonteverdiImages/monteverdi_optical_calibration_outputs.png}
  \itkcaption[Output of the optical calibration module]{Optical calibration module's outputs.}
  \label{fig:opticalcalibrationoutput}
\end{figure}

\subsection{Pan-sharpening}\label{ssec:pxs}

Because of physical constrains on the sensor design, it is difficult
to achieve high spatial and spectral resolution at the same time : a
better spatial resolution means a smaller detector, which in turns
means lesser optical flow on the detector surface. On the contrary,
spectral bands are obtained through filters applied on the detector
surface, that lowers the optical flow, so that it is necessary to
increase the detector size to achieve an acceptable signal to noise
ratio.

For these reasons, many high resolution satellite payload are composed
of two sets of detectors, which in turns delivers two different kind
of images :

\begin{itemize}
\item The multi-spectral (XS) image, composed of 3 to 8 spectral bands
  containing usually blue, green, red and near infra-red bands at a
  given resolution (usually from 2.8 meters to 2 meters).
\item The panchromatic (PAN) image, which is a grayscale image acquired by a
  detector covering a wider part of the light spectrum, which allows
  to increase the optical flow and thus to reduce pixel
  size. Therefore, resolution of the panchromatic image is usually
  around 4 times lower than the resolution of the multi-spectral image
  (from 46 centimeters to 70 centimeters).
\end{itemize}

It is very frequent that those two images are delivered side by side
by data providers. Such a dataset is called a bundle. A very common
remote sensing processing is to fuse the panchromatic image with the
multi-spectral one so as to get an image combining the spatial
resolution of the panchromatic image with the spectral richness of the
multi-spectral image. This operation is called pan-sharpening.

This fusion operation requires two different steps :
\begin{enumerate}
\item The multi-spectral (XS) image is zoomed and registered to the
  panchromatic image,
\item A pixel-by-pixel fusion operator is applied to the co-registered
  pixels of the multi-spectral and panchromatic image to obtain the
  fused pixels.
\end{enumerate}

Using either \app or modules from \mont, it is
possible to perform both steps in a row, or step-by-step fusion, as
described in the above sections.

\subsubsection{Pan-sharpening with \app}

The \application{BundleToPerfectSensor} application allows to
perform both steps in a row. Seamless sensor modelling is used to
perform zooming and registration of the multi-spectral image on the
panchromatic image. Then, a simple pan-sharpening is applied,
according to the following formula:

\begin{equation}
PXS(i,j) = \frac{PAN(i,j)}{PAN_{smooth}(i,j)} \cdot XS(i,j)
\end{equation}

Where $i$ and $j$ are pixels indices, $PAN$ is the panchromatic image,
$XS$ is the multi-spectral image and $PAN_{smooth}$ is the
panchromatic image smoothed with a kernel to fit the multi-spectral
image scale.

Here is a simple example of how to use the
\application{BundleToPerfectSensor} application:

\begin{verbatim}
otbcli_BundleToPerfectSensor -inp pan_image -inxs xs_image -out output_image
\end{verbatim}

There are two more optional parameters that can be useful for this
tool:
\begin{itemize}
\item The \verb?-elev? option allows to specify the elevation, either with
  a DEM formatted for OTB (\verb?-elev.dem? option, see
  section~\ref{ssec:dem}) or with an average elevation (\verb?-elev.default?
  option). Since registration
  and zooming of the multi-spectral image is performed using
  sensor-models, it may happen that the registration is not perfect in
  case of landscape with high elevation variation. Using a DEM in this
  case allows to get better registration.
% todo
% (see \ref{ssec:registration} to learn about how to lower registration error).
\item The \verb?-lmSpacing? option allows to specify the step of the
  registration grid between the multi-spectral image and panchromatic
  image. This is expressed in amount of panchromatic pixels. A lower
  value gives a more precise registration but implies more computation
  with the sensor models, and thus increase the computation
  time. Default value is 10 pixels, which gives sufficient precision
  in most of the cases.
\end{itemize}

Pan-sharpening is a quite heavy processing requiring a lot of system
resource. The \verb?-ram? option allows you to limit the amount of memory
available for the computation, and to avoid overloading your
computer. Increasing the available amount of RAM may also result in
better computation time, seems it optimises the use of the system
resources. Default value is 256 Mb.


\subsubsection{Pan-sharpening with \mont}

\mont allows to perform step-by-step fusion. The followings screenshots highlight operations needed to perform Pan-Sharpening.

\begin{itemize}
\item Open panchromatic and multispectral images in monteverdi using the \mmod{Open Dataset} module or using the  \verb?-il? option of the \mont executable.

\item The \mmod{Superimpose} module is used to zoomed and registered
  the multispectral on the panchromatic image. As a result, we get a
  multispectral dataset with the same geographic extension and the
  same resolution as the panchromatic image, cf ~\ref{fig:qbmulsuper}.

\begin{figure}
  \center
  \includegraphics[width=0.45\textwidth]{../Art/MonteverdiImages/monteverdi_QB_PAN_ROI.png}
  \includegraphics[width=0.45\textwidth]{../Art/MonteverdiImages/monteverdi_QB_MUL_Superimpose.png}
  \itkcaption[Input panchromatic and zoomed and registered multispectral image over it]{Panchromatic, Zoomed, and registered multispectral image.}
  \label{fig:qbmulsuper}
\end{figure}

\item Now the \mmod{Simple RCS pan-sharpening} module can be used using the panchromatic and the multispectral images as inputs. It produces a multispectral image with the same resolution and geographic extension (cf ~\ref{fig:pansharpen}).

\begin{figure}
  \center
  \includegraphics[width=0.6\textwidth]{../Art/MonteverdiImages/monteverdi_QB_XS_pan-sharpened.png}
  \itkcaption[Pan-sharpened image]{Pan-sharpened image using the simple RCS module.}
  \label{fig:pansharpen}
\end{figure}

\end{itemize}


Please also note that since registration and zooming of the
multi-spectral image with the panchromatic image relies on sensor
modelling, this tool will work only for images whose sensor models is
available in \otb (see section~\ref{ssec:ortho} for a detailed
list). It will also work with ortho-ready products in cartographic
projection.

\subsection{Digital Elevation Model management}\label{ssec:dem}

A Digital Elevation Model (DEM) is an georeferenced image (collection
of images) of the elevation. DEM are useful for tasks involving sensor
to ground and ground to sensor coordinate transforms like
ortho-rectification (see section~\ref{ssec:ortho}). In facts, these
transforms need to find the intersection between the line of sight of
the sensor and the earth geoid. If a simple spheroid is used as the
earth model, potentially high localisation errors can be made in areas
where elevation is high or perturbed. Of course, DEM accuracy and
resolution have a great impact on the precision of these transforms.

There exists two main available DEM free of charges with worldwide
cover, which are both delivered as 1-by-1 tiles:
\begin{itemize}
\item \href{http://www2.jpl.nasa.gov/srtm/}{The Shuttle Radar
  topographic Mission (SRTM)} is a 90 meters resolution DEM, obtained
  by radar interferometry during a campaign of the Endeavour space
  shuttle from NASA in 2000.
\item The \href{http://www.ersdac.or.jp/GDEM/E/2.html}{Advanced
  Spaceborne Thermal Emission and Reflection Radiometer (ASTER)} is a
  30 meters resolution DEM obtained by stereoscopic processing of the
  archive of the ASTER instrument.
\end{itemize}

The \otb suite relies on \ossim capabilities for sensor modelling and
DEM handling. Tiles of a given DEM are supposed to be located within a
single directory. Whenever some applications or modules from
\mont require a DEM, the option \textbf{elev.dem} allows set the DEM
directory

For some type DEM for example, you need also use a geoid to manage elevation
properly. For this, you need to specify a path to a file which contains the
geoid. Geoid corresponds to the equipotential surface that would coincide with
the mean ocean surface of the Earth (see
\href{http://en.wikipedia.org/wiki/Geoid}). We provide one geoid in the OTB-Data
repository available
\href{http://hg.orfeo-toolbox.org/OTB-Data/file/4722d9e672c6/Input/DEM/egm96.grd}{here}.

In all applications, the option \textbf{elev.geoid} allows to manage the path to
the geoid. Finally, it is also possible to use an average elevation in case no
DEM is available by using the \textbf{elev.default} option.

%
%\subsubsection{Making DEM files usable in \otb suite using \application{DEMConvert}}
%
%Georeferenced files containing elevation data (including file from
%SRTM and ASTER) need to be converted to a specific file format before
%being compatible with \ossim and \otb. The
%\application{DEMConvert} application allows to perform this
%conversion with the following command:
%
%\begin{verbatim}
%otbcli_DEMConvert -in input_image -out output_prefix
%\end{verbatim}
%
%The output files can then be placed in the directory containing DEM
%files.
%

%\subsubsection{Extract the DEM image corresponding to your image using
%  \mont}


\subsection{Ortho-rectification and map projections}\label{ssec:ortho}

There are several level of products available on the remote sensing
imagery market. The most basic level often provide the geometry of
acquisition (sometimes called the raw geometry). In this case, pixel
coordinates can not be directly used as geographical positions. For
most sensors (but not for all), the different lines corresponds to
different acquisition times and thus different sensor positions, and
different rows correspond to different cells of the detector.

The mapping of a raw image so as to be registered to a cartographic
grid is called ortho-rectification, and consist in inverting the
following effects (at least):
\begin{itemize}
\item In most cases, lines are orthogonal to the sensor trajectory,
 which is not exactly (and in some case not at all) following a
 north-south axis,
\item Depending on the sensor, the line of sight may be different from
  a Nadir (ground position of the sensor), and thus a projective
  warping may appear,
\item The variation of height in the landscape may result in severe
  warping of the image.
\end{itemize}

Moreover, depending on the area of the world the image has been
acquired on, different map projections should be used.

The ortho-rectification process is as follows: once an appropriate map
projection has been defined, a localisation grid is computed to map
pixels from the raw image to the ortho-rectified one. Pixels from the
raw image are then interpolated according to this grid in order to
fill the ortho-rectified pixels.

Ortho-rectification can be performed either with \app or \mont. Sensor
parameters and image meta-data are seamlessly read from the image
files without needing any user interaction, provided that all
auxiliary files are available. The sensor for which \otb
supports ortho-rectification of raw products are the following:
\begin{itemize}
\item Pleiades,
\item SPOT5,
\item Ikonos,
\item Quickbird,
\item GeoEye,
\item WorldView.
\end{itemize}

In addition, GeoTiff and other file format with geographical
information are seamlessly read by \otb, and the ortho-rectification
tools can be used to re-sample these images in another map projection.

\subsubsection{Ortho-rectification with \app}

The \application{OrthoRectification} application allows to
perform ortho-rectification and map re-projection. The simplest way to
use it is the following command:
\begin{verbatim}
otbcli_OrthoRectification -io.in input_image -io.out output_image
\end{verbatim}

In this case, the tool will automatically estimates all the necessary
parameters:
\begin{itemize}
\item The map projection is set to UTM (a worldwide map projection)
  and the UTM zone is automatically estimated,
\item The ground sampling distance of the output image is computed to
  fit the image resolution,
\item The region of interest (upper-left corner and size of the image)
  is estimated so as to contain the whole input image extent.
\end{itemize}

In order to use a Digital Elevation Model (see section~\ref{ssec:dem})
for better localisation performances, one can pass the directory
containing the DEM tiles to the application:

\begin{verbatim}
otbcli_OrthoRectification -io.in input_image
                          -io.out output_image
                          -elev.dem dem_dir
\end{verbatim}

If one wants to use a different map projection, the \emph{-map}
option may be used (example with \emph{lambert93} map projection):

\begin{verbatim}

otbcli_OrthoRectification -io.in input_image
                          -io.out output_image
                          -elev.dem dem_dir
                          -map lambert93

\end{verbatim}

Map projections handled by the application are the following
(please note that the ellipsoid is always WGS84):
\begin{itemize}
\item UTM : \verb?-map utm? \\
The UTM zone and hemisphere can be set by the options \verb?-map.utm.zone? and \verb?-map.utm.northhem?.
\item Lambert 2 etendu: \verb?-map lambert2?
\item Lambert 93: \verb?-map lambert93?
\item TransMercator: \verb?-map transmercator? \\
The related parameters (false easting, false northing and scale factor) can be set by the
options \verb?-map.transmercator.falseeasting?, \verb?-map.transmercator.falsenorthing? and
\verb?-map.transmercator.scale?
\item WGS : \verb?-map wgs?
\item Any map projection system with an EPSG code : \verb?-map epsg? \\
The EPSG code is set with the option \verb?-map.epsg.code?
\end{itemize}

The group \verb?outputs? contains parameters to set the origin, size and spacing of the output image.
For instance, the ground spacing can be specified as follows:

\begin{verbatim}

otbcli_OrthoRectification -io.in input_image
                          -io.out output_image
                          -elev.dem dem_dir
                          -map lambert93
                          -outputs.spacingx spx
                          -outputs.spacingy spy

\end{verbatim}

Please note that since the y axis of the image is bottom oriented, the
y spacing should be negative to avoid switching north and south direction.

A user-defined region of interest to ortho-rectify can be specified as
follows:

\begin{verbatim}

otbcli_OrthoRectification -io.in input_image
                          -io.out output_image
                          -elev.dem dem_dir
                          -map lambert93
                          -outputs.spacingx spx
                          -outputs.spacingy spy
                          -outputs.ulx ul_x_coord
                          -outputs.uly ul_y_coord
                          -outputs.sizex x_size
                          -outputs.sizey y_size

\end{verbatim}

Where the \verb?-outputs.ulx? and \verb?-outputs.uly? options allow to
specify the coordinates of the upper-left corner of the output image.
The \verb?-outputs.sizex? and \verb?-outputs.sizey? options allow to
specify the size of the output image.

A few more interesting options are available:
\begin{itemize}
\item The \verb?-opt.rpc? option allows to use an estimated RPC model
  instead of the rigorous SPOT5 model, which speeds-up the processing,
\item The \verb?-opt.gridspacing? option allows to define the spacing of the
  localisation grid used for ortho-rectification. A coarser grid
  results in speeding-up the processing, but with potential loss of
  accuracy. A standard value would be 10 times the ground spacing of
  the output image.
\item The \verb?-interpolator? option allows to change the interpolation
  algorithm between nearest neighbor, linear and bicubic. Default is nearest
  neighbor interpolation, but bicubic should be fine in most cases.
\item The \verb?-opt.ram? option allows to specify the amount of memory
  available for the processing (in Mb). Default is 256 Mb. Increasing
  this value to fit the available memory on your computer might
  speed-up the processing.
\end{itemize}

%\subsubsection{Ortho-rectification with \mont}
%todo

\subsection{Residual registration}\label{ssec:registration}

Image registration is a fundamental problem in image processing. The aim is to
align two or more images of the same scene often taken at different times, from
different viewpoints, or by different sensors. It is a basic step for
orthorectification, image stiching, image fusion, change detection\ldots

Sensor model is generally not sufficient to provide image registrations. Indeed,
several sources of geometric distortion can be contained in optical remote
sensing images including earth rotation, platform movement, nonlinearity\ldots

They result in geometric errors on scene level, image level and pixel level. It
is critical to rectify the errors before a thematic map is generated, especially
when the remote sensing data need to be integrated together with other GIS data.

This figure illustrates the generic workflow to register image series:
%add figure
\begin{center}
     \begin{tikzpicture}[scale=0.2]
    \tiny
    \draw[fill=black!10] (-1,-12) rectangle (75,17);
     \foreach \x in {5,...,1}
       \draw[fill=red] (\x,\x) rectangle +(4,4);
     \node[fill=black!10, text width= 1.5cm] (InputSeries) at
       (4,-1) {Input series};
     %\pause
     \draw[->,thick] (9,5) --  +(3,0);
     %%\pause
     \draw[fill=black!30,rounded corners=2pt] (12.2,3) rectangle +(6,4);
     \node[text width= 0.8cm] (SensorModel) at (15,5) {Sensor Model};
     %\pause
     \draw[fill=red!30] (1,-10) rectangle +(4,4);
     \node[fill=black!10, text width= 1.2cm] (DEM) at
       (5,-11) {DEM};
     %\pause
     \draw[->,thick] (3,-5.5) --  ++(0,3) -- ++(12,0) -- ++(0,5);
     %\pause
     \draw[->,thick] (18.5,5) --  +(3,0);
     %\pause
     \foreach \x in {5,...,1}
       \draw[fill=blue,xshift=600pt] (\x,\x) rectangle +(4,4);
     \node[fill=black!10, text width= 2.8cm] (GeoRefSeries) at
       (28,-1) {Geo-referenced Series};
%\pause


       \draw[->,thick] (25.5,8.5) --  +(0,3);

     \draw[fill=black!30,rounded corners=2pt] (22,12) rectangle +(8.5,4);
     \node[text width= 1.5cm] (HomPoExtr) at (27,14) {Homologous Points};

     \draw[->,thick] (21.5,14) --  +(-2.5,0);

     \draw[fill=black!30,rounded corners=2pt] (11,12) rectangle +(8,4);
     \node[text width= 1.3cm] (BBAdj) at (15.5,14) {Bundle-block Adjustement};

     \draw[->,thick] (15,11.5) --  +(0,-4);

     %\pause
      \draw[->,thick] (30,5) --  +(3,0);
      %\pause
     \draw[fill=black!30,rounded corners=2pt] (33.2,2.5) rectangle +(6,4.5);
     \node[text width= 0.7cm] (FineRegistration) at (36,4.9) {Fine Registration};
     %\pause


     \draw[->,thick] (39.5,5) --  +(3,0);
     %\pause
     \foreach \x in {5,...,1}
       \draw[fill=green,xshift=1200pt] (\x,\x) rectangle +(4,4);
     \node[fill=black!10, text width= 1.8cm] (RegistSeries) at
       (47,-1) {Registered Series};
     %\pause
     \draw[->,thick] (36,2) --  ++(0,-10) -- ++(-30,0);

     %\pause
      \draw[->,thick] (52,5) --  +(3,0);
      %\pause
     \draw[fill=black!30,rounded corners=2pt] (55.2,2.5) rectangle +(6,4.5);
     \node[text width= 0.7cm] (CartoProjection) at (57.5,4.9)
          {Map Projection};
     %\pause


     \draw[->,thick] (61.5,5) --  +(3,0);
     %\pause
     \foreach \x in {5,...,1}
       \draw[fill=yellow,xshift=1810pt] (\x,\x) rectangle +(4,4);
     \node[fill=black!10, text width= 1.95cm] (CartoSeries) at
       (68,-1) {Cartographic Series};


     \end{tikzpicture}
     \end{center}


We will now illustrate this process by applying this workflow to register two
images. This process can be easyly extended to perform image series
registration.


\subsubsection{Extract metadata from the image reference}

We first dump geometry metadata of the image we want to refine in a text
file. In OTB, we use the extension \textit{.geom} for this type of file. As you
will see the application wich will estimate a refine geometry only needs as
input this metadata and a set of homologous points.

\begin{verbatim}

otbcli_ReadImageInfo   -in slave_image
                       -outkwl slave_image.geom

\end{verbatim}


\subsubsection{Extract homologous points from images}

The main idea of the residual registration is to estimate an second
transformation (after the application of sensors model).

The basic idea is to use image pixels in both images to find a set of homologous
points and estimate with them a residual transformation which will be use to
superimpose.

So the first step is to compute homologous points between images using
keypoints. SIFT or SURF keypoints can be used and the band on which keypoints
are computed can be set independantly for both images. These methods have bthe
advantage to be invariant to uniform scaling, orientation, and partially
invariant to affine distortion and illumination changes

The application offers two modes : the first is the full mode where keypoints
are extracted from the full extent of both images (please note that in this mode
large image file are not supported). The second mode, called \textit{geobins},
allows to set-up spatial binning so as to get fewer points spread accross the
entire image. In this mode, the corresponding spatial bin in the second image is
estimated using geographical transform or sensor modelling, and is padded
according to the user defined precision. We will use this second mode in the
case of image registration.

Last, in both modes the application can filter matches whose colocalisation in
the first image exceed this precision. The elevation parameters allow to deal
more precisely with sensor modelling in case of sensor geometry data. The
\textit{outvector} option allows to create a vector file with segments
corresponding to the localisation error between the matches.
%todo let 2wgs84
\begin{verbatim}


otbcli_HomologousPointsExtraction   -in1 slave_image
                                    -in2 reference_image
                                    -algorithm.surf
                                    -mode geobins
                                    -mode.geobins.binstep 512
                                    -mode.geobins.binsize 512
                                    -mfilter 1
                                    -precision 20
                                    -2wgs84 1
                                    -out homologous_points.txt
                                    -outvector points.shp
                                    -elev.dem dem_path/SRTM4-HGT/
                                    -elev.geoid OTB-Data/Input/DEM/egm96.grd

\end{verbatim}

Note that for a proper use of the application, elevation must be correctly set
(including DEM and geoid file).

\subsubsection{Geometry refinement using homologous points}

Now that we can use this set of tie points to estimate a residual transformation (affine)

For this we use the dedicated application called \textbf{RefineSensorModel}.

This application reads the input geometry metadata file which contains the
sensor model information that we want to refine and a text file containing a
list of ground control point, and performs a least-square fit of the sensor
model adjustable parameters to these tie points. It produces an
updated geometry file as output (the extension always use is \textit{.geom})

The application can provide as well as an optional ground control points based
statistics file and a vector file containing residues.

Please note again that for a proper use of the application, elevation must be
correctly set (including DEM and geoid file). The map parameters allows to
choose a map projection in which the accuracy will be estimated (in meters).

Accuracy (computed using tie points location) allows to control the precision of
the estimated model.

\begin{verbatim}

otbcli_RefineSensorModel   -elev.dem dem_path/SRTM4-HGT/
                           -elev.geoid OTB-Data/Input/DEM/egm96.grd
                           -ingeom slave_image.geom
                           -outgeom refined_slave_image.geom
                           -inpoints homologous_points.txt
                           -outstat stats.txt
                           -outvector refined_slave_image.shp

\end{verbatim}

\subsubsection{Superimpose images using affine geometry}

Now we will show how we can use this new sensor model. In our case we'll use
this sensor model to superimpose the image over the reference. \otb offers since
version 3.16 the possibility to extend image path to use different metadata file
as input. That's what we are going to use there to superimpose the second image
over the first one using for the second image estimated sensor model which take
into account the original sensor model of the slave and which also fit to the
set of tie points.

TODO : explain or link to an explanation on how works extended file name in OTB

\begin{verbatim}

otbcli_Superimpose   -inr reference_image
                     -inm slave_image?&refined_slave_image.geom
                     -elev.dem dem_path/SRTM4-HGT/
                     -elev.geoid OTB-Data/Input/DEM/egm96.grd
                     -out superimpose_slave_image

\end{verbatim}

As a result, if you've got enough homologous points in images and control that
the residual error between the set of tie points and the estimated sensor model
is small, you must achieve a good registration now between the 2 images, normally far better thant 'only' performing seperate orthorectification over the 2 images.

This methodology can be adapt and apply in several cases, for example :
\begin{itemize}

\item to register stereo pair of images and estimate accurate epipolar geometry (see todo)
\item to refine orthorectified images using a reference image (like IGN orthophoto)

\end{itemize}
